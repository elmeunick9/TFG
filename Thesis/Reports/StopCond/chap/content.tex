\newcommand{\VS}{\vspace{6pt}}
\newcommand{\vt}[1]{\vec{#1}}

\chapter{Stop Condition} % Main chapter title

\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

This modification intends to find the optimal number of features that still performs reasonably well in terms of accuracy (i.e. stop condition) in a non-expensive manner. 

\section{Description and reasoning}
\label{sec:stopCond.desc}

For SVM-RFE usually two stop conditions are considered, one is to calculate all feature subsets until a single feature remains. The second is to use a parameter to specify the number of desired features.

\subsubsection*{Using all feature subsets}

Calculating all feature subsets and then trying to find the best one is expensive. However, this process allows us to define a trade-off and find all Pareto optimal solutions by the method of scalarization as discussed in section [m4.1.3]. This pro\-cess also allows us to plot the hyperplane (in our case a red line) indicating all points that, if feasible, would also be Pareto optimal.

\subsubsection*{Using a feature subset size parameter}

Using a desired feature subset size parameter is usually inconvenient. The best situation would be if that size is a re\-quire\-ment, but often we rather have a preference (e.g. prioritize accuracy). It is impossible to know which is the optimal size without running the algorithm, but we need to know that before running it. This is similar to the chicken and the egg problem\footnote{If something is a chicken and egg problem, it is impossible to deal with a problem because the solution is also the cause of the problem.}.

This kind of problems can usually be solved by approximation, i.e. we first run a simplified, computationally inexpensive, version of SVM-RFE (e.g. resampling or increasing the step) and we deduce the optimal size form that. Once the optimal size is known (approximately), we can run the full SVM-RFE version.

Note that implementing this stop condition is trivial, we only need to modify the exit condition in line 4 of [Algorithm \ref{alg:svmrfe-stopcond}] such that we exit if $|\vt{s}| > p$ with $p$ being the subset size parameter.

\subsubsection*{Alternative}

Here we propose a more efficient alternative. As discussed in section [m4.4.1] the ranking criteria is an indicative of the performance of the classification when re\-mov\-ing some feature $f_i$, in other words, the feature importance. Thus, we can determine a stop condition such that when important features are selected for being removed (their ranking criteria value is big) we stop iterating.

The actual limit can be indicated with a parameter, a threshold, this one being a subjective question similar to that of the trade-off between accuracy and feature subset size.

It would also be interesting to investigate whether we can actually estimate the final accuracy using the ranking criteria and from there perform linear scalarization in the same way that we did when using all feature subsets. We could then compare if both methods give similar results.

\section{Pseudocode formalization}

\textbf{Definitions:}

\begin{itemize}
    \item $X_0 = [\vt{x_0}, \vt{x_1}, \dotsc, \vt{x_k}]^T$ list of observations.
    \item $\vt{y} = [y_1, y_2, \dotsc, y_k]^T$ list of labels.
\end{itemize}

\begin{algorithm}[H]
    \DontPrintSemicolon
      \KwInput{$t, t_0$ \tcp*{$t$ = step, $t_0$ = threshold, $0 \le t_0$}}
      \KwOutput{$\vt{r}$}
      \KwData{$X_0,\vt{y}$}
      $\vt{s} = [1,2, \dotsc, n]$ \tcp*{subset of surviving features}
      $\vt{r} = []$ \tcp*{feature ranked list}
      $q = 0$ \tcp*{stop condition}
      \While{$|\vt{s}| > 0 \land t_0 > q$}
        {
            \tcc*[h]{Restrict training examples to good feature indices}\\
            $X=X_0(:,\vt{s})$\VS

            \tcc*[h]{Train the classifier}\\
            $\vt{\alpha} = \texttt{SVM-train(} X, y \texttt{)}$\VS

            \tcc*[h]{Compute the weight vector of dimension length $|\vt{s}|$}\\
            $\vt{w} = \sum_k{\vt{\alpha_k} \vt{y_k} \vt{x_k}}$\VS

            \tcc*[h]{Compute the ranking criteria}\\
            $\vt{c} = [(w_i)^2 \text{ for all $i$}]$\VS

            \tcc*[h]{Find the $t$ features with the smallest ranking criterion}\\
            $\vt{f} = \texttt{argsort}(\vt{c})(\ :t)$\VS

            \tcc*[h]{Iterate over the feature subset}\\
            \For{$f_i \in \vt{f}$}{
                \tcc*[h]{Store last weight eliminated}\\
                $q = \vt{c}(f_i)$\VS
    
                \tcc*[h]{Update the feature ranking list}\\
                $\vt{r} = [\vt{s}(f_i), ...\vt{r}]$\VS
    
                \tcc*[h]{Eliminate the feature selected}\\
                $\vt{s} = [...\vt{s}(1:f_i - 1), ...\vt{s}(f_i + 1:|\vt{s}|)]$
            }
        }
    \caption{SVM-RFE with Stop Condition}
    \label{alg:svmrfe-stopcond}
\end{algorithm}

\section{Results}

