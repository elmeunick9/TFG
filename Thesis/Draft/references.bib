
@article{guyon_gene_2002,
	title = {Gene {Selection} for {Cancer} {Classification} using {Support} {Vector} {Machines}},
	volume = {46},
	issn = {1573-0565},
	doi = {10.1023/A:1012487302797},
	abstract = {DNA micro-arrays now permit scientists to screen thousands of genes simultaneously and determine whether those genes are active, hyperactive or silent in normal or cancerous tissue. Because these new micro-array devices generate bewildering amounts of raw data, new analytical methods must be developed to sort out whether cancer tissues have distinctive signatures of gene expression over normal tissues or other types of cancer tissues.},
	language = {en},
	number = {1},
	journal = {Machine Learning},
	author = {Guyon, Isabelle and Weston, Jason and Barnhill, Stephen and Vapnik, Vladimir},
	month = jan,
	year = {2002},
	pages = {389--422},
	file = {Springer Full Text PDF:C\:\\Users\\Maou\\Zotero\\storage\\WMQUJMBF\\Guyon et al. - 2002 - Gene Selection for Cancer Classification using Sup.pdf:application/pdf},
}

@article{saeys_review_2007,
	title = {A review of feature selection techniques in bioinformatics},
	volume = {23},
	issn = {1367-4803},
	doi = {10.1093/bioinformatics/btm344},
	abstract = {Feature selection techniques have become an apparent need in many bioinformatics applications. In addition to the large pool of techniques that have already been developed in the machine learning and data mining fields, specific applications in bioinformatics have led to a wealth of newly proposed techniques.In this article, we make the interested reader aware of the possibilities of feature selection, providing a basic taxonomy of feature selection techniques, and discussing their use, variety and potential in a number of both common as well as upcoming bioinformatics applications.Contact:yvan.saeys@psb.ugent.beSupplementary information:http://bioinformatics.psb.ugent.be/supplementary\_data/yvsae/fsreview},
	number = {19},
	journal = {Bioinformatics},
	author = {Saeys, Yvan and Inza, Iñaki and Larrañaga, Pedro},
	month = oct,
	year = {2007},
	pages = {2507--2517},
	file = {Full Text PDF:C\:\\Users\\Maou\\Zotero\\storage\\6LMUICI8\\Saeys et al. - 2007 - A review of feature selection techniques in bioinf.pdf:application/pdf;Snapshot:C\:\\Users\\Maou\\Zotero\\storage\\9QEVELTZ\\185254.html:text/html},
}

@article{li_feature_2017,
	title = {Feature {Selection}: {A} {Data} {Perspective}},
	volume = {50},
	issn = {0360-0300},
	shorttitle = {Feature {Selection}},
	url = {https://doi.org/10.1145/3136625},
	doi = {10.1145/3136625},
	abstract = {Feature selection, as a data preprocessing strategy, has been proven to be effective and efficient in preparing data (especially high-dimensional data) for various data-mining and machine-learning problems. The objectives of feature selection include building simpler and more comprehensible models, improving data-mining performance, and preparing clean, understandable data. The recent proliferation of big data has presented some substantial challenges and opportunities to feature selection. In this survey, we provide a comprehensive and structured overview of recent advances in feature selection research. Motivated by current challenges and opportunities in the era of big data, we revisit feature selection research from a data perspective and review representative feature selection algorithms for conventional data, structured data, heterogeneous data and streaming data. Methodologically, to emphasize the differences and similarities of most existing feature selection algorithms for conventional data, we categorize them into four main groups: similarity-based, information-theoretical-based, sparse-learning-based, and statistical-based methods. To facilitate and promote the research in this community, we also present an open source feature selection repository that consists of most of the popular feature selection algorithms (http://featureselection.asu.edu/). Also, we use it as an example to show how to evaluate feature selection algorithms. At the end of the survey, we present a discussion about some open problems and challenges that require more attention in future research.},
	number = {6},
	urldate = {2021-03-01},
	journal = {ACM Comput. Surv.},
	author = {Li, Jundong and Cheng, Kewei and Wang, Suhang and Morstatter, Fred and Trevino, Robert P. and Tang, Jiliang and Liu, Huan},
	month = dec,
	year = {2017},
	keywords = {Feature selection},
	pages = {94:1--94:45},
	file = {Full Text PDF:C\:\\Users\\Maou\\Zotero\\storage\\9N99BN2E\\Li et al. - 2017 - Feature Selection A Data Perspective.pdf:application/pdf},
}

@article{guyon_introduction_2003,
	title = {An {Introduction} to {Variable} and {Feature} {Selection}},
	volume = {3},
	issn = {1533-7928},
	abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better deﬁnition of the objective function, feature construction, feature ranking, multivariate feature selection, efﬁcient search methods, and feature validity assessment methods.},
	number = {Mar},
	journal = {Journal of Machine Learning Research},
	author = {Guyon, Isabelle and Elisseeff, André},
	year = {2003},
	pages = {1157--1182},
	file = {Full Text PDF:C\:\\Users\\Maou\\Zotero\\storage\\JCIGW6GC\\Guyon and Elisseeff - 2003 - An Introduction to Variable and Feature Selection.pdf:application/pdf},
}

@techreport{noauthor_salarios_2017,
	title = {Salarios, ingresos, cohesión social},
	url = {https://www.ine.es/jaxiT3/Tabla.htm?t=10911},
	abstract = {Salario medio anual por sectores de actividad económica y periodo. M Actividades profesionales, científicas y técnicas. Hombres y Mujeres. Periodo 2016-2017.},
	institution = {Encuestas de Estructura Salarial. INE},
	year = {2017},
}

@article{rakotomamonjy_variable_2003,
	title = {Variable selection using svm based criteria},
	volume = {3},
	issn = {1532-4435},
	abstract = {We propose new methods to evaluate variable subset relevance with a view to variable selection. Relevance criteria are derived from Support Vector Machines and are based on weight vector {\textbar}{\textbar}w{\textbar}{\textbar}2 or generalization error bounds sensitivity with respect to a variable. Experiments on linear and non-linear toy problems and real-world datasets have been carried out to assess the effectiveness of these criteria. Results show that the criterion based on weight vector derivative achieves good results and performs consistently well over the datasets we used.},
	number = {null},
	journal = {J. Mach. Learn. Res.},
	author = {Rakotomamonjy, Alain},
	month = mar,
	year = {2003},
	pages = {1357--1370},
	file = {Full Text PDF:C\:\\Users\\Maou\\Zotero\\storage\\UAIXP3IJ\\Rakotomamonjy - 2003 - Variable selection using svm based criteria.pdf:application/pdf},
}

@inproceedings{wang_classification_2011,
	title = {Classification of lip color based on multiple {SVM}-{RFE}},
	doi = {10.1109/BIBMW.2011.6112469},
	abstract = {Classification of lip color is an important aspect in the theory of Traditional Chinese Medicine (TCM). The lip color of one person can reflect the person's healthy status. This paper investigates the effectiveness of multiple support vector machine recursive feature elimination (SVM-RFE) for feature selection in the classification of lip color. In the proposed method, both the normalized histogram features and the mean/variance features are computed for the ranking score from a statistical analysis of weight vectors of multiple linear SVMs trained on subsamples of the original training data. Experimental results show that not only the multiple SVM-RFE is effective for feature selection in the lip color classification, but also the accuracy rate of classification of the proposed method is better than the existing SVM method, which is close up to 91\%.},
	author = {Wang, Jingjing and Li, Xiaoqiang and Fan, Huafu and Li, Fufeng},
	month = nov,
	year = {2011},
	pages = {769--772},
	file = {Full Text PDF:C\:\\Users\\Maou\\Zotero\\storage\\3594HPYT\\Wang et al. - 2011 - Classification of lip color based on multiple SVM-.pdf:application/pdf},
}

@article{huang_svm-rfe_2014,
	title = {{SVM}-{RFE} {Based} {Feature} {Selection} and {Taguchi} {Parameters} {Optimization} for {Multiclass} {SVM} {Classifier}},
	volume = {2014},
	issn = {2356-6140},
	url = {https://www.hindawi.com/journals/tswj/2014/795624/},
	doi = {10.1155/2014/795624},
	abstract = {Recently, support vector machine (SVM) has excellent performance on classification and prediction and is widely used on disease diagnosis or medical assistance. However, SVM only functions well on two-group classification problems. This study combines feature selection and SVM recursive feature elimination (SVM-RFE) to investigate the classification accuracy of multiclass problems for Dermatology and Zoo databases. Dermatology dataset contains 33 feature variables, 1 class variable, and 366 testing instances; and the Zoo dataset contains 16 feature variables, 1 class variable, and 101 testing instances. The feature variables in the two datasets were sorted in descending order by explanatory power, and different feature sets were selected by SVM-RFE to explore classification accuracy. Meanwhile, Taguchi method was jointly combined with SVM classifier in order to optimize parameters and to increase classification accuracy for multiclass classification. The experimental results show that the classification accuracy can be more than 95\% after SVM-RFE feature selection and Taguchi parameter optimization for Dermatology and Zoo databases.},
	language = {en},
	urldate = {2021-03-18},
	journal = {The Scientific World Journal},
	author = {Huang, Mei-Ling and Hung, Yung-Hsiang and Lee, W. M. and Li, R. K. and Jiang, Bo-Ru},
	month = sep,
	year = {2014},
	note = {Publisher: Hindawi},
	pages = {e795624},
	file = {Full Text PDF:C\:\\Users\\Maou\\Zotero\\storage\\BPE9UJ4L\\Huang et al. - 2014 - SVM-RFE Based Feature Selection and Taguchi Parame.pdf:application/pdf;Snapshot:C\:\\Users\\Maou\\Zotero\\storage\\2TF7U923\\795624.html:text/html},
}

@article{xue_nonlinear_2018,
	title = {Nonlinear feature selection using {Gaussian} kernel {SVM}-{RFE} for fault diagnosis},
	volume = {48},
	issn = {1573-7497},
	url = {https://doi.org/10.1007/s10489-018-1140-3},
	doi = {10.1007/s10489-018-1140-3},
	abstract = {Feature selection can directly ascertain causes of faults by selecting useful features for fault diagnosis, which can simplify the procedures of fault diagnosis. As an efficient feature selection method, the linear kernel support vector machine recursive feature elimination (SVM-RFE) has been successfully applied to fault diagnosis. However, fault diagnosis is not a linear issue. Thus, this paper introduces the Gaussian kernel SVM-RFE to extract nonlinear features for fault diagnosis. The key issue is the selection of the kernel parameter for the Gaussian kernel SVM-RFE. We introduce three classical and simple kernel parameter selection methods and compare them in experiments. The proposed fault diagnosis framework combines the Gaussian kernel SVM-RFE and the SVM classifier, which can improve the performance of fault diagnosis. Experimental results on the Tennessee Eastman process indicate that the proposed framework for fault diagnosis is an advanced technique.},
	language = {en},
	number = {10},
	urldate = {2021-03-18},
	journal = {Appl Intell},
	author = {Xue, Yangtao and Zhang, Li and Wang, Bangjun and Zhang, Zhao and Li, Fanzhang},
	month = oct,
	year = {2018},
	pages = {3306--3331},
	file = {Springer Full Text PDF:C\:\\Users\\Maou\\Zotero\\storage\\DR9SQXZT\\Xue et al. - 2018 - Nonlinear feature selection using Gaussian kernel .pdf:application/pdf},
}

@inproceedings{mundra_svm-rfe_2007,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{SVM}-{RFE} with {Relevancy} and {Redundancy} {Criteria} for {Gene} {Selection}},
	isbn = {978-3-540-75286-8},
	doi = {10.1007/978-3-540-75286-8_24},
	abstract = {This paper introduces a novel gene selection method incorporating mutual information in the support vector machine recursive feature elimination (SVM-RFE). We incorporate an additional term of mutual information based minimum redundancy maximum relevancy criteria along with feature weight calculated by SVM algorithm. We tested proposed method on colon cancer and leukemia cancer gene expression dataset. The results show that the proposed method performs better than the original SVM-RFE method. The selected gene subset has better classification accuracy and better generalization capability.},
	language = {en},
	booktitle = {Pattern {Recognition} in {Bioinformatics}},
	publisher = {Springer},
	author = {Mundra, Piyushkumar A. and Rajapakse, Jagath C.},
	editor = {Rajapakse, Jagath C. and Schmidt, Bertil and Volkert, Gwenn},
	year = {2007},
	keywords = {cancer classification, Gene selection, maximum relevancy, minimum redundancy, mutual information, SVM-RFE},
	pages = {242--252},
	file = {Springer Full Text PDF:C\:\\Users\\Maou\\Zotero\\storage\\BZS7LF8K\\Mundra and Rajapakse - 2007 - SVM-RFE with Relevancy and Redundancy Criteria for.pdf:application/pdf},
}

@inproceedings{guyon_result_2004,
	title = {Result {Analysis} of the {NIPS} 2003 {Feature} {Selection} {Challenge}},
	volume = {17},
	abstract = {The NIPS 2003 workshops included a feature selection competi- tion organized by the authors. We provided participants with five datasets from dierent application domains and called for classifica- tion results using a minimal number of features. The competition took place over a period of 13 weeks and attracted 78 research groups. Participants were asked to make on-line submissions on the validation and test sets, with performance on the validation set being presented immediately to the participant and performance on the test set presented to the participants at the workshop. In total 1863 entries were made on the validation sets during the development period and 135 entries on all test sets for the final competition. The winners used a combination of Bayesian neural networks with ARD priors and Dirichlet diusion trees. Other top entries used a variety of methods for feature selection, which com- bined filters and/or wrapper or embedded methods using Random Forests, kernel methods, or neural networks as a classification en- gine. The results of the benchmark (including the predictions made by the participants and the features they selected) and the scor- ing software are publicly available. The benchmark is available at www.nipsfsc.ecs.soton.ac.uk for post-challenge submissions to stimulate further research.},
	author = {Guyon, Isabelle and Gunn, Steve and Ben-Hur, Asa and Dror, Gideon},
	month = jan,
	year = {2004},
	file = {Full Text PDF:C\:\\Users\\Maou\\Zotero\\storage\\KXL5VETB\\Guyon et al. - 2004 - Result Analysis of the NIPS 2003 Feature Selection.pdf:application/pdf},
}

@misc{ertam_internet_2019,
	title = {Internet {Firewall} {Data} {Data} {Set}},
	url = {https://archive.ics.uci.edu/ml/datasets/Internet+Firewall+Data},
	urldate = {2021-03-25},
	journal = {UCI Machine Learning Repository},
	author = {Ertam, Fatih},
	month = feb,
	year = {2019},
	file = {UCI Machine Learning Repository\: Internet Firewall Data Data Set:C\:\\Users\\Maou\\Zotero\\storage\\T4GA45MH\\Internet+Firewall+Data.html:text/html},
}

@article{ding_improving_2006,
	title = {Improving the {Performance} of {SVM}-{RFE} to {Select} {Genes} in {Microarray} {Data}},
	volume = {7},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/1471-2105-7-S2-S12},
	doi = {10.1186/1471-2105-7-S2-S12},
	abstract = {Recursive Feature Elimination is a common and well-studied method for reducing the number of attributes used for further analysis or development of prediction models. The effectiveness of the RFE algorithm is generally considered excellent, but the primary obstacle in using it is the amount of computational power required.},
	language = {en},
	number = {2},
	urldate = {2021-03-28},
	journal = {BMC Bioinformatics},
	author = {Ding, Yuanyuan and Wilkins, Dawn},
	month = sep,
	year = {2006},
	pages = {S12},
	file = {Springer Full Text PDF:C\:\\Users\\Maou\\Zotero\\storage\\48CC4UMI\\Ding and Wilkins - 2006 - Improving the Performance of SVM-RFE to Select Gen.pdf:application/pdf},
}

@misc{noauthor_wikipedia_2021,
	title = {Wikipedia - {Line} (geometry),},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Line_(geometry)&oldid=1010000668},
	language = {en},
	urldate = {2021-04-17},
	journal = {Wikipedia},
	month = apr,
	year = {2021},
	note = {Page Version ID: 1010000668},
	file = {Snapshot:C\:\\Users\\Maou\\Zotero\\storage\\XKFRN8FJ\\index.html:text/html},
}

@book{bishop_pattern_2006,
	title = {Pattern {Recognition} and {Machine} {Learning}},
	isbn = {0-387-31073-8},
	publisher = {Microsoft Research Ltd},
	author = {Bishop, Christopher},
	year = {2006},
	file = {Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf:C\:\\Users\\Maou\\Documents\\Books\\TFG\\Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf:application/pdf},
}

@inproceedings{bai_automatic_2018,
	title = {Automatic {Device} {Classification} from {Network} {Traffic} {Streams} of {Internet} of {Things}},
	doi = {10.1109/LCN.2018.8638232},
	author = {Bai, Lei and Yao, Lina and Kanhere, Salil and Wang, Xianzhi and Yang, Zheng},
	month = oct,
	year = {2018},
	pages = {1--9},
	file = {Full Text PDF:C\:\\Users\\Maou\\Zotero\\storage\\3CT74PEI\\Bai et al. - 2018 - Automatic Device Classification from Network Traff.pdf:application/pdf},
}

@misc{bisong_machine_2021,
	title = {Machine {Learning}: {An} {Overview}},
	shorttitle = {Machine {Learning}},
	url = {https://ekababisong.org//ieee-ompi-workshop/ml-overview/},
	abstract = {Learning Systems, Machine Learning, Deep Neural Networks, Deep Reinforcement Learning},
	urldate = {2021-04-17},
	journal = {dvdbisong.github.io},
	author = {Bisong, Ekaba},
	year = {2021},
	file = {Snapshot:C\:\\Users\\Maou\\Zotero\\storage\\2SZL8LS6\\ml-overview.html:text/html},
}

@article{claesen_hyperparameter_2015,
	title = {Hyperparameter {Search} in {Machine} {Learning}},
	url = {http://arxiv.org/abs/1502.02127},
	abstract = {We introduce the hyperparameter search problem in the field of machine learning and discuss its main challenges from an optimization perspective. Machine learning methods attempt to build models that capture some element of interest based on given data. Most common learning algorithms feature a set of hyperparameters that must be determined before training commences. The choice of hyperparameters can significantly affect the resulting model's performance, but determining good values can be complex; hence a disciplined, theoretically sound search strategy is essential.},
	urldate = {2021-04-18},
	journal = {arXiv:1502.02127 [cs, stat]},
	author = {Claesen, Marc and De Moor, Bart},
	month = apr,
	year = {2015},
	note = {arXiv: 1502.02127},
	keywords = {Computer Science - Machine Learning, G.1.6, I.2.6, I.2.8, I.5, Statistics - Machine Learning},
	annote = {Comment: 5 pages, accepted for MIC 2015: The XI Metaheuristics International Conference in Agadir, Morocco},
	file = {arXiv Fulltext PDF:C\:\\Users\\Maou\\Zotero\\storage\\PD7GRP8B\\Claesen and De Moor - 2015 - Hyperparameter Search in Machine Learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Maou\\Zotero\\storage\\JBQ6APIQ\\1502.html:text/html},
}

@misc{noauthor_lecture_nodate,
	title = {Lecture 16: {Learning}: {Support} {Vector} {Machines} {\textbar} {Lecture} {Videos} {\textbar} {Artificial} {Intelligence} {\textbar} {Electrical} {Engineering} and {Computer} {Science} {\textbar} {MIT} {OpenCourseWare}},
	shorttitle = {Lecture 16},
	url = {https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/lecture-videos/lecture-16-learning-support-vector-machines/},
	abstract = {In this lecture, we explore support vector machines in some mathematical detail. We use Lagrange multipliers to maximize the width of the street given certain constraints.  If needed, we transform vectors into another space, using a kernel function.},
	language = {en},
	urldate = {2021-04-18},
	file = {Snapshot:C\:\\Users\\Maou\\Zotero\\storage\\KCE8VFEI\\lecture-16-learning-support-vector-machines.html:text/html},
}

@article{chapelle_training_2007,
	title = {Training a {Support} {Vector} {Machine} in the {Primal}},
	volume = {19},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.2007.19.5.1155},
	doi = {10.1162/neco.2007.19.5.1155},
	abstract = {Most literature on support vector machines (SVMs) concentrates on the dual optimization problem. In this letter, we point out that the primal problem can also be solved efficiently for both linear and nonlinear SVMs and that there is no reason for ignoring this possibility. On the contrary, from the primal point of view, new families of algorithms for large-scale SVM training can be investigated.},
	number = {5},
	urldate = {2021-04-19},
	journal = {Neural Computation},
	author = {Chapelle, Olivier},
	month = may,
	year = {2007},
	pages = {1155--1178},
	file = {Full Text PDF:C\:\\Users\\Maou\\Zotero\\storage\\KYNNQCST\\Chapelle - 2007 - Training a Support Vector Machine in the Primal.pdf:application/pdf;Snapshot:C\:\\Users\\Maou\\Zotero\\storage\\JCJS5WJ9\\Training-a-Support-Vector-Machine-in-the-Primal.html:text/html},
}

@book{deisenroth_mathematics_2020,
	title = {Mathematics {For} {Machine} {Learning}},
	publisher = {Cambridge University Press},
	author = {Deisenroth, Marc Peter and Faisal, A. Aldo and {Cheng Soon Ong}},
	year = {2020},
	file = {mml-book.pdf:C\:\\Users\\Maou\\Documents\\Books\\TFG\\mml-book.pdf:application/pdf},
}

@misc{noauthor_wikipedia_2021-1,
	title = {Wikipedia - {Hilbert} space,},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Hilbert_space&oldid=1017590047},
	language = {en},
	urldate = {2021-04-26},
	journal = {Wikipedia},
	month = apr,
	year = {2021},
	note = {Page Version ID: 1017590047},
	file = {Snapshot:C\:\\Users\\Maou\\Zotero\\storage\\PSQ5UZPK\\index.html:text/html},
}

@misc{bernstein_radial_2017,
	title = {The {Radial} {Basis} {Function} {Kernel}},
	url = {http://pages.cs.wisc.edu/~matthewb/pages/notes/pdf/svms/RBFKernel.pdf},
	urldate = {2021-04-27},
	author = {Bernstein, Matthew N.},
	month = mar,
	year = {2017},
	file = {RBFKernel.pdf:C\:\\Users\\Maou\\Zotero\\storage\\BLMGJ4ZJ\\RBFKernel.pdf:application/pdf},
}

@article{vasumathi_scalarizing_2019,
	title = {Scalarizing functions in solving multi-objective problem-an evolutionary approach},
	volume = {13},
	copyright = {Copyright (c) 2018 Institute of Advanced Engineering and Science},
	issn = {2502-4760},
	url = {http://ijeecs.iaescore.com/index.php/IJEECS/article/view/14164},
	doi = {10.11591/ijeecs.v13.i3.pp974-981},
	abstract = {Scalarizing functions had long been observed for optimization of multi-objective problems. Scalarizing functions on multi-objective problem along with Differential Evolution (DE) algorithm variants had been used to analyze the effect of scalarizing functions. The main purpose is to find the better scalarizing function which can be applied for optimization. The effective solution of the multi-objective problem depends on the various factors like the DE algorithm and the scalarizing functions used. Multi objective evolutionary algorithm (MOEA) framework in java had been used for performing the analysis. The Obtained results showed that Tchebysheff scalarization function performs better than the other scalarizing functions on various indicator functions used.},
	language = {en},
	number = {3},
	urldate = {2021-04-30},
	journal = {Indonesian Journal of Electrical Engineering and Computer Science},
	author = {Vasumathi, D. and S, Thangavelu},
	month = mar,
	year = {2019},
	note = {Number: 3},
	keywords = {Differential evolution, Indicator functions, MOEA framework, Multi-objective problems, scalarizing functions},
	pages = {974--981},
	file = {Full Text PDF:C\:\\Users\\Maou\\Zotero\\storage\\6CWP5ANR\\Vasumathi and S - 2019 - Scalarizing functions in solving multi-objective p.pdf:application/pdf;Snapshot:C\:\\Users\\Maou\\Zotero\\storage\\XDD28DNT\\14164.html:text/html},
}

@article{guyon_results_nodate,
	title = {{RESULTS} {OF} {THE} {NIPS} 2003 {FEATURE} {SELECTION} {CHALLENGE}},
	language = {en},
	author = {Guyon, Isabelle and Gunn, Steve and Hur, Asa Ben and Dror, Gideon},
	pages = {36},
	file = {Guyon et al. - RESULTS OF THE NIPS 2003 FEATURE SELECTION CHALLEN.pdf:C\:\\Users\\Maou\\Zotero\\storage\\W4HJ9TKE\\Guyon et al. - RESULTS OF THE NIPS 2003 FEATURE SELECTION CHALLEN.pdf:application/pdf},
}

@article{sontag_kernel_2013,
	title = {Kernel methods \& optimization},
	url = {https://people.csail.mit.edu/dsontag/courses/ml14/slides/lecture3_notes.pdf},
	language = {en},
	author = {Sontag, David},
	month = sep,
	year = {2013},
	pages = {5},
	file = {Sontag - 1 Kernel methods & optimization.pdf:C\:\\Users\\Maou\\Zotero\\storage\\6ME7Z5F8\\Sontag - 1 Kernel methods & optimization.pdf:application/pdf},
}

@article{abdiansah_time_2015,
	title = {Time {Complexity} {Analysis} of {Support} {Vector} {Machines} ({SVM}) in {LibSVM}},
	volume = {128},
	issn = {09758887},
	url = {http://www.ijcaonline.org/research/volume128/number3/abdiansah-2015-ijca-906480.pdf},
	doi = {10.5120/ijca2015906480},
	abstract = {Support Vector Machines (SVM) is one of machine learning methods that can be used to perform classification task. Many researchers using SVM library to accelerate their research development. Using such a library will save their time and avoid to write codes from scratch. LibSVM is one of SVM library that has been widely used by researchers to solve their problems. The library also integrated to WEKA, one of popular Data Mining tools. This article contain results of our work related to complexity analysis of Support Vector Machines. Our work has focus on SVM algorithm and its implementation in LibSVM. We also using two popular programming languages i.e C++ and Java with three different dataset to test our analysis and experiment. The results of our research has proved that the complexity of SVM (LibSVM) is O(n3) and the time complexity shown that C++ faster than Java, both in training and testing, beside that the data growth will be affect and increase the time of computation.},
	language = {en},
	number = {3},
	urldate = {2021-06-14},
	journal = {IJCA},
	author = {Abdiansah, Abdiansah and Wardoyo, Retantyo},
	month = oct,
	year = {2015},
	pages = {28--34},
	file = {Abdiansah and Wardoyo - 2015 - Time Complexity Analysis of Support Vector Machine.pdf:C\:\\Users\\Maou\\Zotero\\storage\\NBFDY7EM\\Abdiansah and Wardoyo - 2015 - Time Complexity Analysis of Support Vector Machine.pdf:application/pdf},
}
