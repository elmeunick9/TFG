% Chapter 1

\chapter{Introduction} % Main chapter title

\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

This bachelor thesis at the Computer Engineering Degree, specialization in Com\-puting, has been done in the Facultat d’Informàtica de Barcelona of the Universitat Politècnica de Catalunya (UPC) and directed by Luis Antonio Belanche Muñoz, PhD. in Computer Science.

\section{Context}
\label{sec:context}

In statistics, machine learning, data-mining, and other related disciplines, it is often the case that there are redundant or irrelevant data in a dataset\footnote{A table with rows / records / observations and columns / variables / features / dimensions / predictors / attributes.}. Indeed, before we can start working with the data, some form of data analysis and cleaning is required. Data cleaning may include removing duplicated rows or rows with missing values, removing observations that are clearly outliers, removing irrelevant variables (e.g. name, surname, email address), etc.

With the new era of Big Data, datasets have increased in size, both in number of observations and in dimensions. Applying classical data-mining and machine learning algorithms to these high-dimensional data raise multiple issues collectively known as “the curse of dimensionality”. One such issue is the elevated, usually intractable, cost and memory requirements derived from the non-linear (on number of dimensions and observations) complexities of the algorithms. Another issue has to do with data in a high-dimensional space becoming sparse and negatively af\-fect\-ing the performance of algorithms designed to work in a low-dimensional space. And finally, a third issue is that with a high number of dimensions the algorithms tend to overfit, that is, they don't generalize enough and end up producing models that perform worse with real data than their predicted performance with the training data. (\cite{li_feature_2017})

Simple manual data cleaning is not enough to achieve satisfactory amounts of dimensionality reduction. In this case we can use automatic techniques. We can classify such techniques in two categories: feature extraction and feature selection.

\subsection{Feature Extraction}

Feature extraction techniques transform the original high-dimensional space into a new low-dimensional space by extracting or deriving information from the original features. The premise is to compress the data in order to pack the same information at the expense of model explainability\footnote{The ability to explain why certain predictions are made. Also, interpretability.}. Continuing with our data compression analogy, virtually all feature extraction techniques perform lossy com\-pres\-sion. That is, some information is lost which makes the process irreversible. 

Some well known feature extraction algorithms include Principal Component Analysis (PCA) (\cite{scholkopf_kernel_1997}) and auto-encoders (\cite{vincent_extracting_2008}), the first being a transformation over the feature-space and the second a neuronal network. PCA may be extended with a kernel method in order to make non-linear transformations. Similarly, we will also use non-linear kernels in some of our extensions.

\subsection{Feature Selection}
\label{sec:ch1.feature_selection}

In contrast, feature selection only selects a subset of the existing features, ideally the most relevant or useful. This may imply a greater loss of information compared to feature extraction, but it doesn't reduce explainability. Some problems require feature selection explicitly. In domains such as genetic analysis and text mining, feature selection is not necessarily used to build predictors. For example in micro\-array analysis feature selection is used to identify genes (i.e. features) that dis\-criminate between healthy and sick patients. 

Note that the number of possible selections of all sizes is $2^m$. This is the same as the amount of subsets of a set with size $m$, where $m$ is the amount of total features. The number of possible selections for some feature subset size $k$, is then ${m \choose k} = \frac{m!}{k!(m - k)!}$. This makes the optimal feature selection via exhaustive search intractable. Instead, we use methods that do not require an exhaustive search, such as a greedy algorithm, at the expense of not being able to guarantee an optimal solution.

Feature selection methods may be classified by how they are constructed in three categories:

\begin{itemize}
    \item \textbf{Filters:} A \emph{feature ranking criterion} is used to sort the features in order of rel\-e\-vance, then select the $k$-most relevant.
    \item \textbf{Wrappers:} They use a learning machine (treated as a black box) to train and validate the dataset with different subsets of variables. They rank the subsets based on the performance (score) of the model. A wide range of learning machines and search strategies can be used. Within the greedy strategies we find \emph{forward selection} and \emph{backward elimination}.
    \item \textbf{Embedded:} Like wrapper methods but more efficient. They use information from the model itself at training time to make feature selection. Because they don't use the score, they can also skip testing the model.
\end{itemize}

In both wrapper and embedded methods greedy strategies can be used. \textbf{SVM-RFE} is a feature selection algorithm, of the embedded class, that uses Support Vector Machines (SVM) and a greedy strategy called Recursive Feature Elimination (RFE). This algorithm is an instance of backward elimination. It starts with a set of all features and eliminates the less relevant each iteration. Within each iteration SVM-RFE behaves like a filter method and uses a feature ranking criterion to decide which feature to eliminate. SVM-RFE takes advantage of the fact that, for linear SVM, the ranking criterion is to take the variable with the smallest absolute weight in each iteration. Where the weights are the co\-ef\-fi\-cients of the hyperplane resulting from the SVM training.  (\cite{guyon_introduction_2003})

%----------------------------------------------------------------------------------------

\section{State of the art}

The SVM-RFE algorithm was first proposed in a paper on the topic of cancer class\-ification (\cite{guyon_gene_2002}). This paper uses the SVM-RFE algorithm to identify highly discriminant genes, encoded as features, that have plausible relevance to cancer diagnosis. Since then, SVM-RFE has remained a popular technique for gene selection and the original paper cited more than four thousand times.

The paper already proposes some natural extensions to the al\-go\-rithm, such as eliminating multiple features each iteration based on the ranking and a \emph{step} constant. Further research has been on improving and extending different parts of the al\-go\-rithm. These include the use of a Gaussian kernel (\cite{xue_nonlinear_2018}), using multiple SVM in the same iteration (\cite{wang_classification_2011}) or simply trying to find a better ranking criterion (\cite{mundra_svm-rfe_2007}).

%----------------------------------------------------------------------------------------

\section{Project goals}
\label{sec:ch1.objective}

The main objective of this project is to research possible extensions and im\-prove\-ments that try to optimize the SVM-RFE algorithm. Optimizations may be in the form of improved performance or a reduction in time utilization. We do not propose here any optimizations that may affect space complexity. As such, we also skip the space complexity analysis for the whole project. We've classified the possible extensions as follows.

\subsubsection*{Main extensions}

These focus on improving the performance of the selection of features.

\begin{itemize}
    \item \textbf{Non-linear kernel:} Use a more general ranking criterion and apply it to handle SVM with arbitrary kernels.
    \item \textbf{Multi-class criteria:} Find a ranking criterion that can handle multiple weight vectors in a useful way.
\end{itemize}

\subsubsection*{Secondary extensions}

Focus on reducing the computational cost.

\begin{itemize}
    \item \textbf{Internal sampling:} Use a different subset of the observations on each iteration.
    \item \textbf{Dynamic step:} Instead of using a constant value in each iteration, calculate it dynamically.
    \item \textbf{Stop condition:} Determine the amount of features that are relevant (SVM-RFE only provides a ranking) in an effective and inexpensive manner.
\end{itemize}

\subsubsection*{Combination of various extensions}

\begin{itemize}
    \item \textbf{Combo:} Mix \emph{internal sampling}, \emph{dynamic step} and \emph{non-linear Kernels} into a single implementation.
\end{itemize}

\subsection{Goals break down}

To accomplish these goals, the project has been sub\-divided in two parts, each having specific tasks for each extension:

\subsubsection*{Algorithmic Part}

\begin{itemize}
    \item Do research in SVM-RFE and in the extensions that will be tackled in the project.
    \item {
        For each extension:
        \begin{itemize}
            \item Design the algorithm and write its formalization in pseudocode.
            \item Define the expected advantages or disadvantages of this extension over the base SVM-RFE.
            \item Compute the time complexity.
        \end{itemize}
    }
\end{itemize}

\subsubsection*{Practical Part}

\begin{itemize}
    \item Program the base SVM-RFE algorithm and the extensions.
    \item {
        For each extension:
        \begin{itemize}
            \item Analyze its behavior for artificial data sets.
            \item Analyze its behavior for real-world data sets.
        \end{itemize}
    }
    \item Compare the results obtained with the ones expected.
    \item Combine multiple extensions to further improve the algorithm.
    \item Draw conclusions about all the results obtained in the project.
\end{itemize}

%----------------------------------------------------------------------------------------

\subsection{Stakeholders}

This project is intended to be of use for many involved parties. The most directly involved group, is the tutor and the researcher. Luis Antonio Belanche Muñoz is the tutor of this project. Robert Planas Jimenez would be the researcher. Feature selection algorithms is one of the areas of research of the tutor, and he has wanted to explore extensions to the SVM-RFE algorithm. He will lead and guide the researcher for the correct development of the project. The researcher is responsible for planning, developing and documenting the project, as well as experimenting, analyzing and drawing conclusions.

The other group of interested parties would be stakeholders that do not interact with the project directly but still benefit from it. In the first place we have researchers on the fields of bioinformatics and data mining, that use machine learning methods (specifically, SVM-RFE) for micro-array analysis, text analysis, or other of its popular applications. Indirectly, companies that make use of any findings will also benefit. Finally, the general population may also benefit from better diagnostics and more effective drugs. 

\label{sec:risk}
\subsection{Potential obstacles and risks}

Some obstacles and risks identified that could potentially prevent the correct exec\-ution of the project are:

\begin{itemize}
    \item \textbf{Deadline of the project:} There is a deadline for the delivery of the project. This being a research project however, is considerably hard to estimate how much time tasks will take, or even decide whether a task has been finished or not.
    \item \textbf{Bugs on some libraries:} This is considered of low risk, but is still a possibility that errors on the software package used extend to code, making it work in\-correctly.
    \item \textbf{Insufficient computational power:} Machine learning algorithms, in general, can be very resource intensive. It could be the case that our hardware can not handle some datasets. 
    \item \textbf{Hardware related issues:} A hard drive failure could occur that would end in lost data, or a failure in a router could disconnect us from the internet.
    \item \textbf{Health related issues:} In addition to health issues that can occur at any time without prior notice, we're in the middle of a pandemic.
\end{itemize}


\section{Methodology}

\subsection{Framework}

The methodology that we will use for the project is a combination of Kanban (\cite{matharu_empirical_2015}) and Waterfall (\cite{mahadevan_running_2015}) methodologies. Waterfall will be used to define the general phases of the project, and Kanban for tracking the individual tasks. In Waterfall, tasks can not start until the previous task has been completed, and thus following strict deadlines is important. Phases will not start until the previous phase ends. Each phase will be composed of multiple tasks, which will then be managed by the Kanban method\-ol\-o\-gy.

Kanban is much more flexible than Waterfall. Its principal objective is to manage tasks in a general way, by assigning different statuses to them. Kanban stands out by its simplicity, and will continue to endorse that simplicity by managing the visual representation of the cards in a simple, plain, text formatting. Each card will be in a row, with the first column defining its name and the other its status. The statuses we've considered are:

\begin{itemize}
    \item \textbf{To do:} A basic idea of the task is present.
    \item \textbf{Definition:} The task is in the theoretical part.
    \item \textbf{Implementation:} The task is in the practical part.
    \item \textbf{Completed:} The task is finished.
\end{itemize}

An uppercase letter \texttt{"X"} will mark the current state of any given card. If more granular information is required, other marks may be used instead. For example, to indicate that the task is paused a \texttt{"P"} would be used. To indicate the progress within some stage a percentage would be used. For easy monitoring, this table will be kept in the \texttt{readme.md} file of our \texttt{GitHub} repository.

\subsection{Validation}

We will use a GitHub repository as a tool for version control, which will allow us to share code easily and recover from data lose. The repository will contain both the code for the experiments, each in one subfolder, and code for the documentation. In order to verify the implemented code, it will be tested with multiple data sets. In the practical part, hyper-parameters will be selected using some model validation technique, such as the cross-validation. Each ex\-per\-i\-ment will be done at least 6 times and the average of the results will be the final result. 

Face-to-face meetings with the tutor of the project will be scheduled once every two weeks. In these meetings the current project status will be discussed, and the tasks to do during the following two weeks will be defined. In case of unexpected problems, extraordinary meetings can be arranged.
