
@article{guyon_gene_2002,
	title = {Gene {Selection} for {Cancer} {Classification} using {Support} {Vector} {Machines}},
	volume = {46},
	issn = {1573-0565},
	doi = {10.1023/A:1012487302797},
	abstract = {DNA micro-arrays now permit scientists to screen thousands of genes simultaneously and determine whether those genes are active, hyperactive or silent in normal or cancerous tissue. Because these new micro-array devices generate bewildering amounts of raw data, new analytical methods must be developed to sort out whether cancer tissues have distinctive signatures of gene expression over normal tissues or other types of cancer tissues.},
	language = {en},
	number = {1},
	journal = {Machine Learning},
	author = {Guyon, Isabelle and Weston, Jason and Barnhill, Stephen and Vapnik, Vladimir},
	month = jan,
	year = {2002},
	pages = {389--422},
	file = {Springer Full Text PDF:C\:\\Users\\Maou\\Zotero\\storage\\WMQUJMBF\\Guyon et al. - 2002 - Gene Selection for Cancer Classification using Sup.pdf:application/pdf},
}

@article{saeys_review_2007,
	title = {A review of feature selection techniques in bioinformatics},
	volume = {23},
	issn = {1367-4803},
	doi = {10.1093/bioinformatics/btm344},
	abstract = {Feature selection techniques have become an apparent need in many bioinformatics applications. In addition to the large pool of techniques that have already been developed in the machine learning and data mining fields, specific applications in bioinformatics have led to a wealth of newly proposed techniques.In this article, we make the interested reader aware of the possibilities of feature selection, providing a basic taxonomy of feature selection techniques, and discussing their use, variety and potential in a number of both common as well as upcoming bioinformatics applications.Contact:yvan.saeys@psb.ugent.beSupplementary information:http://bioinformatics.psb.ugent.be/supplementary\_data/yvsae/fsreview},
	number = {19},
	journal = {Bioinformatics},
	author = {Saeys, Yvan and Inza, Iñaki and Larrañaga, Pedro},
	month = oct,
	year = {2007},
	pages = {2507--2517},
	file = {Full Text PDF:C\:\\Users\\Maou\\Zotero\\storage\\6LMUICI8\\Saeys et al. - 2007 - A review of feature selection techniques in bioinf.pdf:application/pdf;Snapshot:C\:\\Users\\Maou\\Zotero\\storage\\9QEVELTZ\\185254.html:text/html},
}

@article{li_feature_2017,
	title = {Feature {Selection}: {A} {Data} {Perspective}},
	volume = {50},
	issn = {0360-0300},
	shorttitle = {Feature {Selection}},
	url = {https://doi.org/10.1145/3136625},
	doi = {10.1145/3136625},
	abstract = {Feature selection, as a data preprocessing strategy, has been proven to be effective and efficient in preparing data (especially high-dimensional data) for various data-mining and machine-learning problems. The objectives of feature selection include building simpler and more comprehensible models, improving data-mining performance, and preparing clean, understandable data. The recent proliferation of big data has presented some substantial challenges and opportunities to feature selection. In this survey, we provide a comprehensive and structured overview of recent advances in feature selection research. Motivated by current challenges and opportunities in the era of big data, we revisit feature selection research from a data perspective and review representative feature selection algorithms for conventional data, structured data, heterogeneous data and streaming data. Methodologically, to emphasize the differences and similarities of most existing feature selection algorithms for conventional data, we categorize them into four main groups: similarity-based, information-theoretical-based, sparse-learning-based, and statistical-based methods. To facilitate and promote the research in this community, we also present an open source feature selection repository that consists of most of the popular feature selection algorithms (http://featureselection.asu.edu/). Also, we use it as an example to show how to evaluate feature selection algorithms. At the end of the survey, we present a discussion about some open problems and challenges that require more attention in future research.},
	number = {6},
	urldate = {2021-03-01},
	journal = {ACM Comput. Surv.},
	author = {Li, Jundong and Cheng, Kewei and Wang, Suhang and Morstatter, Fred and Trevino, Robert P. and Tang, Jiliang and Liu, Huan},
	month = dec,
	year = {2017},
	keywords = {Feature selection},
	pages = {94:1--94:45},
	file = {Full Text PDF:C\:\\Users\\Maou\\Zotero\\storage\\9N99BN2E\\Li et al. - 2017 - Feature Selection A Data Perspective.pdf:application/pdf},
}

@article{guyon_introduction_2003,
	title = {An {Introduction} to {Variable} and {Feature} {Selection}},
	volume = {3},
	issn = {1533-7928},
	abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better deﬁnition of the objective function, feature construction, feature ranking, multivariate feature selection, efﬁcient search methods, and feature validity assessment methods.},
	number = {Mar},
	journal = {Journal of Machine Learning Research},
	author = {Guyon, Isabelle and Elisseeff, André},
	year = {2003},
	pages = {1157--1182},
	file = {Full Text PDF:C\:\\Users\\Maou\\Zotero\\storage\\JCIGW6GC\\Guyon and Elisseeff - 2003 - An Introduction to Variable and Feature Selection.pdf:application/pdf},
}

@techreport{noauthor_salarios_2017,
	title = {Salarios, ingresos, cohesión social},
	url = {https://www.ine.es/jaxiT3/Tabla.htm?t=10911},
	abstract = {Salario medio anual por sectores de actividad económica y periodo. M Actividades profesionales, científicas y técnicas. Hombres y Mujeres. Periodo 2016-2017.},
	institution = {Encuestas de Estructura Salarial. INE},
	year = {2017},
}

@article{rakotomamonjy_variable_2003,
	title = {Variable selection using svm based criteria},
	volume = {3},
	issn = {1532-4435},
	abstract = {We propose new methods to evaluate variable subset relevance with a view to variable selection. Relevance criteria are derived from Support Vector Machines and are based on weight vector {\textbar}{\textbar}w{\textbar}{\textbar}2 or generalization error bounds sensitivity with respect to a variable. Experiments on linear and non-linear toy problems and real-world datasets have been carried out to assess the effectiveness of these criteria. Results show that the criterion based on weight vector derivative achieves good results and performs consistently well over the datasets we used.},
	number = {null},
	journal = {J. Mach. Learn. Res.},
	author = {Rakotomamonjy, Alain},
	month = mar,
	year = {2003},
	pages = {1357--1370},
	file = {Full Text PDF:C\:\\Users\\Maou\\Zotero\\storage\\UAIXP3IJ\\Rakotomamonjy - 2003 - Variable selection using svm based criteria.pdf:application/pdf},
}

@inproceedings{wang_classification_2011,
	title = {Classification of lip color based on multiple {SVM}-{RFE}},
	doi = {10.1109/BIBMW.2011.6112469},
	abstract = {Classification of lip color is an important aspect in the theory of Traditional Chinese Medicine (TCM). The lip color of one person can reflect the person's healthy status. This paper investigates the effectiveness of multiple support vector machine recursive feature elimination (SVM-RFE) for feature selection in the classification of lip color. In the proposed method, both the normalized histogram features and the mean/variance features are computed for the ranking score from a statistical analysis of weight vectors of multiple linear SVMs trained on subsamples of the original training data. Experimental results show that not only the multiple SVM-RFE is effective for feature selection in the lip color classification, but also the accuracy rate of classification of the proposed method is better than the existing SVM method, which is close up to 91\%.},
	author = {Wang, Jingjing and Li, Xiaoqiang and Fan, Huafu and Li, Fufeng},
	month = nov,
	year = {2011},
	pages = {769--772},
	file = {Full Text PDF:C\:\\Users\\Maou\\Zotero\\storage\\3594HPYT\\Wang et al. - 2011 - Classification of lip color based on multiple SVM-.pdf:application/pdf},
}

@article{huang_svm-rfe_2014,
	title = {{SVM}-{RFE} {Based} {Feature} {Selection} and {Taguchi} {Parameters} {Optimization} for {Multiclass} {SVM} {Classifier}},
	volume = {2014},
	issn = {2356-6140},
	url = {https://www.hindawi.com/journals/tswj/2014/795624/},
	doi = {10.1155/2014/795624},
	abstract = {Recently, support vector machine (SVM) has excellent performance on classification and prediction and is widely used on disease diagnosis or medical assistance. However, SVM only functions well on two-group classification problems. This study combines feature selection and SVM recursive feature elimination (SVM-RFE) to investigate the classification accuracy of multiclass problems for Dermatology and Zoo databases. Dermatology dataset contains 33 feature variables, 1 class variable, and 366 testing instances; and the Zoo dataset contains 16 feature variables, 1 class variable, and 101 testing instances. The feature variables in the two datasets were sorted in descending order by explanatory power, and different feature sets were selected by SVM-RFE to explore classification accuracy. Meanwhile, Taguchi method was jointly combined with SVM classifier in order to optimize parameters and to increase classification accuracy for multiclass classification. The experimental results show that the classification accuracy can be more than 95\% after SVM-RFE feature selection and Taguchi parameter optimization for Dermatology and Zoo databases.},
	language = {en},
	urldate = {2021-03-18},
	journal = {The Scientific World Journal},
	author = {Huang, Mei-Ling and Hung, Yung-Hsiang and Lee, W. M. and Li, R. K. and Jiang, Bo-Ru},
	month = sep,
	year = {2014},
	note = {Publisher: Hindawi},
	pages = {e795624},
	file = {Full Text PDF:C\:\\Users\\Maou\\Zotero\\storage\\BPE9UJ4L\\Huang et al. - 2014 - SVM-RFE Based Feature Selection and Taguchi Parame.pdf:application/pdf;Snapshot:C\:\\Users\\Maou\\Zotero\\storage\\2TF7U923\\795624.html:text/html},
}

@article{xue_nonlinear_2018,
	title = {Nonlinear feature selection using {Gaussian} kernel {SVM}-{RFE} for fault diagnosis},
	volume = {48},
	issn = {1573-7497},
	url = {https://doi.org/10.1007/s10489-018-1140-3},
	doi = {10.1007/s10489-018-1140-3},
	abstract = {Feature selection can directly ascertain causes of faults by selecting useful features for fault diagnosis, which can simplify the procedures of fault diagnosis. As an efficient feature selection method, the linear kernel support vector machine recursive feature elimination (SVM-RFE) has been successfully applied to fault diagnosis. However, fault diagnosis is not a linear issue. Thus, this paper introduces the Gaussian kernel SVM-RFE to extract nonlinear features for fault diagnosis. The key issue is the selection of the kernel parameter for the Gaussian kernel SVM-RFE. We introduce three classical and simple kernel parameter selection methods and compare them in experiments. The proposed fault diagnosis framework combines the Gaussian kernel SVM-RFE and the SVM classifier, which can improve the performance of fault diagnosis. Experimental results on the Tennessee Eastman process indicate that the proposed framework for fault diagnosis is an advanced technique.},
	language = {en},
	number = {10},
	urldate = {2021-03-18},
	journal = {Appl Intell},
	author = {Xue, Yangtao and Zhang, Li and Wang, Bangjun and Zhang, Zhao and Li, Fanzhang},
	month = oct,
	year = {2018},
	pages = {3306--3331},
	file = {Springer Full Text PDF:C\:\\Users\\Maou\\Zotero\\storage\\DR9SQXZT\\Xue et al. - 2018 - Nonlinear feature selection using Gaussian kernel .pdf:application/pdf},
}

@inproceedings{mundra_svm-rfe_2007,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{SVM}-{RFE} with {Relevancy} and {Redundancy} {Criteria} for {Gene} {Selection}},
	isbn = {978-3-540-75286-8},
	doi = {10.1007/978-3-540-75286-8_24},
	abstract = {This paper introduces a novel gene selection method incorporating mutual information in the support vector machine recursive feature elimination (SVM-RFE). We incorporate an additional term of mutual information based minimum redundancy maximum relevancy criteria along with feature weight calculated by SVM algorithm. We tested proposed method on colon cancer and leukemia cancer gene expression dataset. The results show that the proposed method performs better than the original SVM-RFE method. The selected gene subset has better classification accuracy and better generalization capability.},
	language = {en},
	booktitle = {Pattern {Recognition} in {Bioinformatics}},
	publisher = {Springer},
	author = {Mundra, Piyushkumar A. and Rajapakse, Jagath C.},
	editor = {Rajapakse, Jagath C. and Schmidt, Bertil and Volkert, Gwenn},
	year = {2007},
	keywords = {cancer classification, Gene selection, maximum relevancy, minimum redundancy, mutual information, SVM-RFE},
	pages = {242--252},
	file = {Springer Full Text PDF:C\:\\Users\\Maou\\Zotero\\storage\\BZS7LF8K\\Mundra and Rajapakse - 2007 - SVM-RFE with Relevancy and Redundancy Criteria for.pdf:application/pdf},
}

@inproceedings{guyon_result_2004,
	title = {Result {Analysis} of the {NIPS} 2003 {Feature} {Selection} {Challenge}},
	volume = {17},
	abstract = {The NIPS 2003 workshops included a feature selection competi- tion organized by the authors. We provided participants with five datasets from dierent application domains and called for classifica- tion results using a minimal number of features. The competition took place over a period of 13 weeks and attracted 78 research groups. Participants were asked to make on-line submissions on the validation and test sets, with performance on the validation set being presented immediately to the participant and performance on the test set presented to the participants at the workshop. In total 1863 entries were made on the validation sets during the development period and 135 entries on all test sets for the final competition. The winners used a combination of Bayesian neural networks with ARD priors and Dirichlet diusion trees. Other top entries used a variety of methods for feature selection, which com- bined filters and/or wrapper or embedded methods using Random Forests, kernel methods, or neural networks as a classification en- gine. The results of the benchmark (including the predictions made by the participants and the features they selected) and the scor- ing software are publicly available. The benchmark is available at www.nipsfsc.ecs.soton.ac.uk for post-challenge submissions to stimulate further research.},
	author = {Guyon, Isabelle and Gunn, Steve and Ben-Hur, Asa and Dror, Gideon},
	month = jan,
	year = {2004},
	file = {Full Text PDF:C\:\\Users\\Maou\\Zotero\\storage\\KXL5VETB\\Guyon et al. - 2004 - Result Analysis of the NIPS 2003 Feature Selection.pdf:application/pdf},
}
